{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }.text_cell_render, .output_text {font-family: Lato;font-size: 18px;line-height: 1.5;}.CodeMirror {font-size: 16px;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"\"\"<style>.container { width:90% !important; }.text_cell_render, .output_text {font-family: Lato;font-size: 18px;line-height: 1.5;}.CodeMirror {font-size: 16px;}</style>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gQXaOBBo1oW"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/AnalyticsVidhya_DataHour_PyTorch/main/images/datahour_cover.png\" width=\"80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/AnalyticsVidhya_DataHour_PyTorch/main/images/datahour_about.png\" width=\"80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QEp40UvmAmPK"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyE2ltG3Auin"
   },
   "source": [
    "**PyTorch** is the **fastest growing** Deep Learning framework and it is also used by **Fast.ai** in its MOOC, [Deep Learning for Coders](https://course.fast.ai/) and its [library](https://docs.fast.ai/).\n",
    "\n",
    "PyTorch is also very *pythonic*, meaning, it feels more natural to use it if you already are a Python developer.\n",
    "\n",
    "Besides, using PyTorch may even improve your health, according to [Andrej Karpathy](https://twitter.com/karpathy/status/868178954032513024) :-)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/AnalyticsVidhya_DataHour_PyTorch/main/images/tweet_karpathy.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSt6oP0rA_FG"
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PPZG7RmuBCv0"
   },
   "source": [
    "There are *many many* PyTorch tutorials around and its documentation is quite complete and extensive. So, **why** should you keep reading this step-by-step tutorial?\n",
    "\n",
    "Well, even though one can find information on pretty much anything PyTorch can do, I missed having a **structured, incremental and from first principles** approach to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "<h3>\n",
    "<ul>\n",
    "    <li>A Simple Problem - Linear Regression</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>PyTorch: tensors, tensors, tensors</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>Dataset and DataLoader: splitting your data into mini-batches</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>Sequential Models</li>\n",
    "</ul>    \n",
    "<ul>\n",
    "    <li>Gradient Descent in 5 easy steps!</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>Loss: aggregating errors into a single value</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>Autograd, your companion for all your gradient needs!</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>Optimizer: learning the parameters step-by-step</li>\n",
    "</ul>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Y3FcRg0qJ0s"
   },
   "source": [
    "## A Simple Problem - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vlYSOAy8smmJ"
   },
   "source": [
    "Most tutorials start with some nice and pretty *image classification problem* to illustrate how to use PyTorch. It may seem cool, but I believe it **distracts** you from the **main goal: how PyTorch works**?\n",
    "\n",
    "For this reason, in this tutorial, I will stick with a **simple** and **familiar** problem: a **linear regression with a single feature x**! It doesn’t get much simpler than that…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large y = b + w x + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also think of it as the **simplest neural network**: one node, one input, one output, linear activation function.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/AnalyticsVidhya_DataHour_PyTorch/master/images/NNs_bias_2.png\" width=\"50%\" height=\"50%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHiXYWCMrGEr"
   },
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7gu_5iGsVrA"
   },
   "source": [
    "Let’s start **generating** some synthetic data: we start with a vector of 100 points for our **feature x** and create our **labels** using **b = 1, w = 2** and some Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-MRtiMeAsos"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l36meXzoop0U"
   },
   "outputs": [],
   "source": [
    "true_b = 1\n",
    "true_w = 2\n",
    "N = 100\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1)\n",
    "epsilon = .1 * np.random.randn(N, 1)\n",
    "y = true_b + true_w * x + epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGWRIovYrsxp"
   },
   "source": [
    "### Train / Validation Split\n",
    "\n",
    "Next, let’s **split** our synthetic data into **train** and **validation** sets, shuffling the array of indices and using the first 80 shuffled points for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6itdbvQXp8Xx"
   },
   "outputs": [],
   "source": [
    "# Shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "colab_type": "code",
    "id": "D4E8Dqaar1bn",
    "outputId": "8ee22423-5276-4f2d-b2a9-e7a425278224"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Generated Data - Validation')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].scatter(x_train, y_train)\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylabel('y')\n",
    "ax[0].set_ylim([1, 3])\n",
    "ax[0].set_title('Generated Data - Train')\n",
    "ax[1].scatter(x_val, y_val, c='r')\n",
    "ax[1].set_xlabel('x')\n",
    "ax[1].set_ylabel('y')\n",
    "ax[1].set_ylim([1, 3])\n",
    "ax[1].set_title('Generated Data - Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-p8xcQKo9pDQ"
   },
   "source": [
    "## PyTorch: tensors, tensors, tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2bKKlGN9BaBa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2usggtDH9r2e"
   },
   "source": [
    "First, we need to cover a **few basic concepts** that may throw you off-balance if you don’t grasp them well enough before going full-force on modeling.\n",
    "\n",
    "In Deep Learning, we see **tensors** everywhere. Well, Google’s framework is called *TensorFlow* for a reason! *What is a tensor, anyway*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwyU03EC-zPj"
   },
   "source": [
    "### Tensors\n",
    "\n",
    "In *Numpy*, you may have an **array** that has **three dimensions**, right? That is, technically speaking, a **tensor**.\n",
    "\n",
    "A **scalar** (a single number) has **zero** dimensions, a **vector has one** dimension, a **matrix has two** dimensions and a **tensor has three or more dimensions**. That’s it!\n",
    "\n",
    "But, to keep things simple, it is commonplace to call vectors and matrices tensors as well — so, from now on, **everything is either a scalar or a tensor**.\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/dvgodoy/AnalyticsVidhya_DataHour_PyTorch/main/images/linear_dogs.jpg)\n",
    "Tensors are just higher-dimensional matrices :-) [Source](http://karlstratos.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rPdIb6pIXrwW"
   },
   "source": [
    "You can create **tensors** in PyTorch pretty much the same way you create **arrays** in Numpy. Using [**tensor()**](https://bit.ly/39DRbFv) you can create either a scalar or a tensor.\n",
    "\n",
    "PyTorch's tensors have equivalent functions as its Numpy counterparts, like: [**ones()**](https://bit.ly/3f9W7TU), [**zeros()**](https://bit.ly/3hKoDgo), [**rand()**](https://bit.ly/2P3sQj5), [**randn()**](https://bit.ly/30b0SIq) and many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "KeV7KOyBUi2S",
    "outputId": "87c42cb8-2e8b-4113-856b-d8ab0bce28d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1416)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[[-0.4337,  0.3671,  0.4893, -1.5758],\n",
      "         [-0.2920, -0.3289,  0.9354, -0.2148],\n",
      "         [-0.5700,  0.8749, -0.2089,  1.2250]],\n",
      "\n",
      "        [[ 1.3436, -0.2849,  1.6559,  0.1980],\n",
      "         [ 1.7245, -0.7681, -1.2700,  1.1032],\n",
      "         [-1.4647,  0.7216, -1.6567,  1.0323]]])\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(3.14159)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "matrix = torch.ones((2, 3), dtype=torch.float)\n",
    "tensor = torch.randn((2, 3, 4), dtype=torch.float)\n",
    "\n",
    "print(scalar)\n",
    "print(vector)\n",
    "print(matrix)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wUp_UkLOcApC"
   },
   "source": [
    "You can get the shape of a tensor using its [**size()**](https://pytorch.org/docs/stable/generated/torch.Tensor.size.html#torch.Tensor.size) method or its **shape** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pumy8ocYbpcC",
    "outputId": "de6392ac-100c-4935-85c7-aa4095347c17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(tensor.size(), tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-9UB154cIsr"
   },
   "source": [
    "You can also reshape a tensor using its [**reshape()**](https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html#torch.Tensor.reshape) or [**view()**](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view) methods.\n",
    "\n",
    "Beware: these methods create a new tensor with the desired shape that **shares the underlying data** with the original tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dd5eSRi-XObM",
    "outputId": "d1a67c41-e177-4280-a280-202491c93135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12]) torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "new_tensor1 = tensor.reshape(2, -1)\n",
    "new_tensor2 = tensor.view(2, -1)\n",
    "print(new_tensor1.shape, new_tensor2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCt37k99g2nm"
   },
   "source": [
    "If you want to copy all data for real, that is, duplicate it in memory, you should use either its [**new_tensor()**](https://pytorch.org/docs/stable/generated/torch.Tensor.new_tensor.html#torch.Tensor.new_tensor) or [**clone()**](https://pytorch.org/docs/stable/generated/torch.clone.html#torch.clone) methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q67_KSCSAAT1"
   },
   "source": [
    "### Loading Data, Devices and CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OfRQIc19AEYA"
   },
   "source": [
    "”*How do we go from Numpy’s arrays to PyTorch’s tensors*”, you ask? \n",
    "\n",
    "That’s what [**as_tensor()**](https://bit.ly/3fa9hjy) is good for. It returns a **CPU tensor**, though.\n",
    "\n",
    "You can also easily **cast** it to a lower precision (32-bit float) using [**float()**](https://pytorch.org/docs/stable/generated/torch.Tensor.float.html#torch.Tensor.float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fgIjFB6gBUuz",
    "outputId": "24ef9c69-e450-4d3b-f6a1-9334627cee3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "x_train_tensor = torch.as_tensor(x_train).float()\n",
    "y_train_tensor = torch.as_tensor(y_train).float()\n",
    "\n",
    "print(type(x_train), type(x_train_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hux68h4oCFt4"
   },
   "source": [
    "“*But I want to use my fancy GPU…*”, you say.\n",
    "\n",
    "No worries, that’s what [**to()**](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to) is good for. It sends your tensor to whatever **device** you specify, including your **GPU** (referred to as `cuda` or `cuda:0`).\n",
    "\n",
    "“*What if I want my code to fallback to CPU if no GPU is available?*”, you may be wondering… \n",
    "\n",
    "PyTorch got your back once more — you can use [**cuda.is_available()**](https://bit.ly/2ZZfOJL) to find out if you have a GPU at your disposal and set your device accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q5qlPSxvCGKf",
    "outputId": "cdc313ee-ec97-46b7-99bc-0c757973beb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)\n",
    "\n",
    "print(type(x_train), type(x_train_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4lsYIvjQDE7O"
   },
   "source": [
    "If you compare the **types** of both variables, you’ll get what you’d expect: `numpy.ndarray` for the first one and `torch.Tensor` for the second one.\n",
    "\n",
    "But where does your nice tensor “live”? In your CPU or your GPU? You can’t say… but if you use PyTorch’s **type()**, it will reveal its **location** — `torch.cuda.FloatTensor` — a GPU tensor in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QbECiJmaC8nt",
    "outputId": "957b59fe-5c8d-4cdc-f5b8-f1f871c27c87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XF0MnXHYDRF4"
   },
   "source": [
    "We can also go the other way around, turning tensors back into Numpy arrays, using [**numpy()**](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html#torch.Tensor.numpy). It should be easy as `x_train_tensor.numpy()` but…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "x1uCOtZvDdT0",
    "outputId": "cd027659-6872-420b-cf5c-b7125f727edd"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx_train_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "x_train_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNKoo-n0DlUP"
   },
   "source": [
    "Unfortunately, Numpy **cannot** handle GPU tensors… you need to make them CPU tensors first using [**cpu()**](https://bit.ly/2OSC1Th)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ouRBUpRaDfUG",
    "outputId": "25379c5c-f648-4bfa-de50-e0ab21dbd938"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77127033],\n",
       "       [0.06355835],\n",
       "       [0.86310345],\n",
       "       [0.02541913],\n",
       "       [0.7319939 ],\n",
       "       [0.07404465],\n",
       "       [0.19871569],\n",
       "       [0.31098232],\n",
       "       [0.47221494],\n",
       "       [0.96958464],\n",
       "       [0.12203824],\n",
       "       [0.77513283],\n",
       "       [0.802197  ],\n",
       "       [0.72960615],\n",
       "       [0.09767211],\n",
       "       [0.18485446],\n",
       "       [0.15601864],\n",
       "       [0.02058449],\n",
       "       [0.9868869 ],\n",
       "       [0.6232981 ],\n",
       "       [0.7080726 ],\n",
       "       [0.5979    ],\n",
       "       [0.9218742 ],\n",
       "       [0.63755745],\n",
       "       [0.2809345 ],\n",
       "       [0.25877997],\n",
       "       [0.11959425],\n",
       "       [0.7290072 ],\n",
       "       [0.94888556],\n",
       "       [0.60754484],\n",
       "       [0.5612772 ],\n",
       "       [0.4937956 ],\n",
       "       [0.18182497],\n",
       "       [0.27134904],\n",
       "       [0.96990985],\n",
       "       [0.21233912],\n",
       "       [0.1834045 ],\n",
       "       [0.8661761 ],\n",
       "       [0.37454012],\n",
       "       [0.29122913],\n",
       "       [0.80839735],\n",
       "       [0.05808361],\n",
       "       [0.83244264],\n",
       "       [0.54269606],\n",
       "       [0.77224475],\n",
       "       [0.88721275],\n",
       "       [0.08849251],\n",
       "       [0.04522729],\n",
       "       [0.59241456],\n",
       "       [0.684233  ],\n",
       "       [0.7132448 ],\n",
       "       [0.03438852],\n",
       "       [0.601115  ],\n",
       "       [0.81546146],\n",
       "       [0.4401525 ],\n",
       "       [0.32518333],\n",
       "       [0.785176  ],\n",
       "       [0.76078504],\n",
       "       [0.4951769 ],\n",
       "       [0.19967379],\n",
       "       [0.9507143 ],\n",
       "       [0.29214466],\n",
       "       [0.13949387],\n",
       "       [0.31171107],\n",
       "       [0.7068573 ],\n",
       "       [0.11586906],\n",
       "       [0.35846573],\n",
       "       [0.00552212],\n",
       "       [0.19598286],\n",
       "       [0.89482737],\n",
       "       [0.45606998],\n",
       "       [0.52475643],\n",
       "       [0.14092423],\n",
       "       [0.06505159],\n",
       "       [0.17052412],\n",
       "       [0.8287375 ],\n",
       "       [0.32533032],\n",
       "       [0.93949896],\n",
       "       [0.33089802],\n",
       "       [0.36636186]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to handle tensors, we can use them to build a..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VcHv7EZ56PR"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GUdT1l_Q6Dx_"
   },
   "source": [
    "In PyTorch, a **dataset** is represented by a regular **Python class** that inherits from the [**Dataset**](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class. You can think of it as a kind of a Python **list of tuples**, each tuple corresponding to **one point (features, label)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EUwVUhKP75zt"
   },
   "source": [
    "### TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0-lPsQf770A"
   },
   "source": [
    "If a dataset is nothing else but a **couple of tensors**, we can use PyTorch’s [**TensorDataset**](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) class.\n",
    "\n",
    "Let's create some tensors out of our training data and use them to build a dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wIjLRR_666H2",
    "outputId": "08e88416-cf08-41b3-d297-b65acc07f5bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.7713]), tensor([2.4745]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "x_train_tensor = torch.as_tensor(x_train).float()\n",
    "y_train_tensor = torch.as_tensor(y_train).float()\n",
    "\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zW1bTD4l7VAE"
   },
   "source": [
    "---\n",
    "\n",
    "Did you notice we built our **training tensors** out of Numpy arrays but we **did not send them to a device**? So, they are **CPU** tensors now! **Why**?\n",
    "\n",
    "We **don’t want our whole training data to be loaded into GPU tensors** because **it takes up space** in our precious **graphics card’s RAM**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9bZZI1U37ylE"
   },
   "source": [
    "OK, fine, but then again, **why** are we building a dataset anyway? We’re doing it because we want to use a…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLvd96o77-Rj"
   },
   "source": [
    "## DataLoader, splitting your data into mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JH_FSh4G8CM-"
   },
   "source": [
    "We use PyTorch’s [**DataLoader**](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class for this job. We tell it which **dataset** to use (the one we just built in the previous section), the desired **mini-batch size** and if we’d like to **shuffle** it or not. That’s it!\n",
    "\n",
    "Our **loader** will behave like an **iterator**, so we can **loop over it** and **fetch a different mini-batch** every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ana8r6AN7_hv"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RHHtcorn8dMq"
   },
   "source": [
    "To retrieve a mini-batch, one can simply run the command below — it will return a list containing two tensors, one for the features, another one for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "sPX5ggEC8bXU",
    "outputId": "50146492-76fe-49a7-cf3d-bc7bd3ca43a5"
   },
   "outputs": [],
   "source": [
    "x_train_batch, y_train_batch = next(iter(train_loader))\n",
    "# we send the mini-batch to a device\n",
    "x_train_batch = x_train_batch.to(device)\n",
    "y_train_batch = y_train_batch.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we loaded our training data into a TensorDataset, and used a DataLoader to generate mini-batches. Now it's time to handle other type of tensors..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pYm2AqND-_T"
   },
   "source": [
    "### Creating Tensor for Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCT2YOYjEGJ_"
   },
   "source": [
    "What distinguishes a *tensor* used for *data* — like the ones we’ve just created — from a **tensor** used as a (*trainable*) **parameter/weight**?\n",
    "\n",
    "The latter tensors require the **computation of its gradients**, so we can **update** their values (the parameters’ values, that is). That’s what the **`requires_grad=True`** argument is good for. It tells PyTorch we want it to compute gradients for us.\n",
    "\n",
    "---\n",
    "\n",
    "<h2><b><i>A tensor for a learnable parameter requires gradient!</i></b></h2>\n",
    "\n",
    "---\n",
    "\n",
    "You may be tempted to create a simple tensor for a parameter and, later on, send it to your chosen device, as we did with our data, right?\n",
    "\n",
    "Actually, you should **assign** tensors to a **device** at the moment of their **creation** to avoid unexpected behaviors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vb866lTxiTiA",
    "outputId": "82d08dde-6858-4f8e-a9ea-4556488e0a94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1940], device='cuda:0', requires_grad=True) tensor([0.1391], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# We can specify the device at the moment of creation - RECOMMENDED!\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works fine for a tiny model like ours, but it isn't practical at all if the model has many parameters. Luckily, we can replace a bunch of individual parameters for a single [**Linear**]() layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.8300], requires_grad=True) Parameter containing:\n",
      "tensor([[0.7645]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "layer = nn.Linear(1, 1)\n",
    "print(layer.bias, layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what **are** layers anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KcS2nS2LxpI"
   },
   "source": [
    "### Layers\n",
    "\n",
    "A **Linear** model can be seen as a **layer** in a neural network.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/AnalyticsVidhya_DataHour_PyTorch/main/images/layer.png\" width=\"50%\" height=\"50%\">\n",
    "</p>\n",
    "\n",
    "In the example above, the **hidden layer** would be `nn.Linear(3, 5)` and the **output layer** would be `nn.Linear(5, 1)`.\n",
    "\n",
    "\n",
    "There are **MANY** different layers that can be uses in PyTorch:\n",
    "- [Convolution Layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
    "- [Pooling Layers](https://pytorch.org/docs/stable/nn.html#pooling-layers)\n",
    "- [Padding Layers](https://pytorch.org/docs/stable/nn.html#padding-layers)\n",
    "- [Non-linear Activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "- [Normalization Layers](https://pytorch.org/docs/stable/nn.html#normalization-layers)\n",
    "- [Recurrent Layers](https://pytorch.org/docs/stable/nn.html#recurrent-layers)\n",
    "- [Transformer Layers](https://pytorch.org/docs/stable/nn.html#transformer-layers)\n",
    "- [Linear Layers](https://pytorch.org/docs/stable/nn.html#linear-layers)\n",
    "- [Dropout Layers](https://pytorch.org/docs/stable/nn.html#dropout-layers)\n",
    "- [Sparse Layers (embbedings)](https://pytorch.org/docs/stable/nn.html#sparse-layers)\n",
    "- [Vision Layers](https://pytorch.org/docs/stable/nn.html#vision-layers)\n",
    "- [DataParallel Layers (multi-GPU)](https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed)\n",
    "- [Flatten Layer](https://pytorch.org/docs/stable/nn.html#flatten)\n",
    "\n",
    "We have just used a **Linear** layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, if we're sticking with a simple and straightforward model where the output of each layer feeds directly into the next, we can line them up using a sequential model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8G3evlAo37Gj"
   },
   "source": [
    "<h2><b><i>Run-of-the-mill layers? Sequential model!</b></i></h2>\n",
    "\n",
    "For **straightforward models**, that use **run-of-the-mill layers**, where the output of a layer is sequentially fed as an input to the next, we can use a, er… [**Sequential**](https://bit.ly/3hRQTxP) model :-)\n",
    "\n",
    "In our case, we would build a Sequential model with a single argument, that is, the Linear layer we used to train our linear regression. The model would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 1\n",
    "n_outputs = 1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Sequential()\n",
    "model.add_module('linear', nn.Linear(n_features, n_outputs, bias=True))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJk570c41MlG"
   },
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlTBJ4H21WSj"
   },
   "source": [
    "**IMPORTANT**: we need to **send our model to the same device where the data is**. If our data is made of GPU tensors, our model must “live” inside the GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIWl41iKdJdP"
   },
   "source": [
    "### state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WU5er3Y4MdO0"
   },
   "source": [
    "The **state_dict()** of a given model is simply a Python dictionary that **maps each layer / parameter to its corresponding tensor**. But only **learnable** parameters are included, as its purpose is to keep track of parameters that are going to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight', tensor([[0.7645]], device='cuda:0')),\n",
       "             ('linear.bias', tensor([0.8300], device='cuda:0'))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.7645]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGp27LxNi5ZL"
   },
   "source": [
    "Now that we know how to create tensors and models that require gradients, let’s see how PyTorch handles them — that’s the role of the…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2SePFR0syOS"
   },
   "source": [
    "## Gradient Descent in 5 easy steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aI0IWVbPtGzw"
   },
   "source": [
    "Gradient descent is the most common **optimization algorithm** in Machine Learning and Deep Learning.\n",
    "\n",
    "The purpose of using gradient descent is **to minimize the loss**, that is, **minimize the errors between predictions and actual values** (and sometimes some other term as well).\n",
    "\n",
    "It goes beyond the scope of this tutorial to fully explain how gradient descent works, but I'll cover the **five basic steps** you'd need to go through to compute it, namely:\n",
    "\n",
    "- Step 0: Random initialize parameters / weights\n",
    "- Step 1: Compute model's predictions - forward pass\n",
    "- Step 2: Compute loss\n",
    "- Step 3: Compute the gradients\n",
    "- Step 4: Update the parameters\n",
    "- Step 5: Rinse and repeat!\n",
    "\n",
    "---\n",
    "\n",
    "If you want to learn more about gradient descent, check the following resources:\n",
    "- [**Linear Regression Simulator**](https://www.mladdict.com/linear-regression-simulator), which goes through the very same steps listed here\n",
    "- [**A Visual and Interactive Guide to the Basics of Neural Networks**](http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)\n",
    "- [**Gradient Descent Algorithms and Its Variants**](https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R04jMPtj2wtg"
   },
   "source": [
    "### Step 0: Initialization\n",
    "\n",
    "Technically, this step is not part of gradient descent, but it is an important step nonetheless.\n",
    "\n",
    "For training a model, you need to **randomly initialize the parameters/weights**.\n",
    "\n",
    "Make sure to *always initialize your random seed* to ensure **reproducibility** of your results. As usual, the random seed is [42](https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_(42)), the *least random* of all random seeds one could possibly choose :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[0.7645]], device='cuda:0')), ('linear.bias', tensor([0.8300], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "# Step 0\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = nn.Sequential()\n",
    "model.add_module('linear', nn.Linear(n_features, n_outputs, bias=True))\n",
    "model.to(device)\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TO1oNfOq5_Oi"
   },
   "source": [
    "### Step 1: Compute Model's Predictions\n",
    "\n",
    "This is the **forward pass** - it simply *computes the model's predictions using the current values of the parameters/weights*. At the very beginning, we will be producing really bad predictions, as we started with random values from Step 0.\n",
    "\n",
    "Predictions for a simple linear regression like ours *could* be computed like this, but that's not practical at all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9047],\n",
       "        [1.5348],\n",
       "        [1.4535]], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model.linear.bias + model.linear.weight * x_train_batch\n",
    "yhat[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our model class instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9047],\n",
       "        [1.5348],\n",
       "        [1.4535]], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model(x_train_batch)\n",
    "yhat[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Even though models have a `forward` method, you should **NOT call the `forward(x)`** method. You should always **call the whole model itself**, as in **`model(x)`** to perform a forward pass and output predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, models have a [**train()**](https://bit.ly/30VW2Ox) method which, somewhat disappointingly, **does NOT perform a training step**. Its only purpose is to **set the model to training mode**. \n",
    "\n",
    "Why is this important? Some models may use mechanisms like [**Dropout**](https://bit.ly/2X7v5pU), for instance, which have **distinct behaviors in training and evaluation phases**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tuXlvnf-qlw_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9047],\n",
       "        [1.5348],\n",
       "        [1.4535]], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1\n",
    "# Computes our model's predicted output\n",
    "model.train()\n",
    "yhat = model(x_train_batch)\n",
    "yhat[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAJChW8Gs2vB"
   },
   "source": [
    "### Step 2: Compute Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFkXT7untW14"
   },
   "source": [
    "There is a subtle but fundamental difference between **error** and **loss**. \n",
    "\n",
    "The **error** is the difference between **actual** and **predicted** computed for a single data point.\n",
    "\n",
    "$$\n",
    "\\Large \\text{error}_i = \\hat{y_i} - y_i\n",
    "$$\n",
    "\n",
    "The **loss**, on the other hand, is some sort of **aggregation of errors for a set of data points**.\n",
    "\n",
    "For a regression problem, the **loss** is given by the **Mean Squared Error (MSE)**, that is, the average of all squared differences between **actual values** (y) and **predictions** (a + bx).\n",
    "\n",
    "$$\n",
    "\\large \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N{\\text{error}_i}^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\large \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N{(\\hat{y_i} - y_i)}^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\large \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N{(b + w x_i - y_i)}^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "It is worth mentioning that, if we **compute the loss** using:\n",
    "- **all points** in the training set (N), we are performing a **batch** gradient descent\n",
    "- a **single point** at each time, it would be a **stochastic** gradient descent\n",
    "- anything else (n) **in-between 1 and N** characterizes a **mini-batch** gradient descent\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/AnalyticsVidhya_DataHour_PyTorch/main/images/gradient_descent.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vF-utTp22es0",
    "outputId": "abba8a14-3810-4529-b2a1-556558d1ced2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7570, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# How wrong is our model? That's the error! \n",
    "error = (yhat - y_train_batch)\n",
    "\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAq8ytYfxK7w"
   },
   "source": [
    "### Loss: aggregating errors into a single value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7f_eHQOxN5b"
   },
   "source": [
    "Computing losses manually isn't great. Luckily, PyTorch got us covered once again. There are many [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) to choose from, depending on the task at hand. Since ours is a regression, we are using the [Mean Square Error (MSE)](https://bit.ly/3hNYn4R) loss.\n",
    "\n",
    "---\n",
    "\n",
    "Notice that `nn.MSELoss` actually **creates a loss function** for us — **it is NOT the loss function itself**. Moreover, you can specify a **reduction method** to be applied, that is, **how do you want to aggregate the results for individual points** — you can average them (reduction=’mean’) or simply sum them up (reduction=’sum’).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eC4EH3rKxL1J",
    "outputId": "fad967b0-1bda-4dc2-8799-ba998d310fd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kcS4vSiTyG7U"
   },
   "source": [
    "We then **use** the created loss function to compute the loss given our **predictions** and our **labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7570, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(yhat, y_train_batch)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bO8K6rePs48M"
   },
   "source": [
    "### Step 3: Compute the Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-RZUTYvzDGB"
   },
   "source": [
    "A **gradient** is a **partial derivative** — *why partial*? Because one computes it with respect to (w.r.t.) a **single parameter**. We have two parameters, **b** and **w**, so we must compute two partial derivatives.\n",
    "\n",
    "A **derivative** tells you *how much* **a given quantity changes** when you *slightly* vary some **other quantity**. In our case, how much does our **MSE** **loss** change when we vary **each one of our two parameters**?\n",
    "\n",
    "The *right-most* part of the equations below is what you usually see in implementations of gradient descent for a simple linear regression. In the **intermediate step**, I show you **all elements** that pop-up from the application of the [chain rule](https://en.wikipedia.org/wiki/Chain_rule), so you know how the final expression came to be.\n",
    "\n",
    "---\n",
    "\n",
    "<h3><i><b>Gradient = how much the LOSS changes if ONE parameter changes a little bit!</b></i></h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UaSuPRY5zxqb"
   },
   "source": [
    "**Gradients**:\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial{\\text{MSE}}}{\\partial{b}} = \\frac{\\partial{\\text{MSE}}}{\\partial{\\hat{y_i}}} \\frac{\\partial{\\hat{y_i}}}{\\partial{b}} = \\frac{1}{N} \\sum_{i=1}^N{2(b + w x_i - y_i)} = 2 \\frac{1}{N} \\sum_{i=1}^N{(\\hat{y_i} - y_i)}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial{\\text{MSE}}}{\\partial{w}} = \\frac{\\partial{\\text{MSE}}}{\\partial{\\hat{y_i}}} \\frac{\\partial{\\hat{y_i}}}{\\partial{w}} = \\frac{1}{N} \\sum_{i=1}^N{2(b + w x_i - y_i) \\cdot x_i} = 2 \\frac{1}{N} \\sum_{i=1}^N{x_i (\\hat{y_i} - y_i)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6coSXbUz4h1F",
    "outputId": "ef8f1c8a-fdf9-4e90-e43d-1ca61d9a00bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.5652, device='cuda:0', grad_fn=<MulBackward0>) tensor(-0.9564, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Computes gradients for both \"b\" and \"w\" parameters\n",
    "b_grad = 2 * error.mean()\n",
    "w_grad = 2 * (x_train_batch * error).mean()\n",
    "print(b_grad, w_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68GypByGi8RO"
   },
   "source": [
    "## Autograd, your companion for all your gradient needs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sPV5It5YjAvL"
   },
   "source": [
    "Autograd is PyTorch’s *automatic differentiation package*. Thanks to it, we **don’t need to worry** about partial derivatives, chain rule or anything like it.\n",
    "\n",
    "<h2><b><i>Computing gradients manually?! No way! Backward!</b></i></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irQuMOgJo6FU"
   },
   "source": [
    "So, how do we tell PyTorch to do its thing and **compute all gradients**? That’s what [**backward()**](https://bit.ly/3eV9Dub) is good for.\n",
    "\n",
    "Do you remember the **starting point** for **computing the gradients**? It was the **loss**, as we computed its partial derivatives w.r.t. our parameters. Hence, we need to invoke the `backward()` method from the corresponding Python variable, like, `loss.backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model(x_train_batch)\n",
    "loss = loss_fn(yhat, y_train_batch)\n",
    "# Step 3    \n",
    "# No more manual computation of gradients! \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "youD0_1Do9_E"
   },
   "source": [
    "### grad / zero_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z2JJ_osngW5H"
   },
   "source": [
    "\n",
    "What about the **actual values** of the **gradients**? We can inspect them by looking at the [**grad**](https://bit.ly/3fYtNFa) **attribute** of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.9129]], device='cuda:0'), tensor([-3.1303], device='cuda:0'))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight.grad, model.linear.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJL6O06ChLio"
   },
   "source": [
    "If you check the method’s documentation, it clearly states that **gradients are accumulated**. \n",
    "\n",
    "So, every time we use the **gradients** to **update** the parameters, we need to **zero the gradients afterwards**. And that’s what [**zero_()**](https://pytorch.org/docs/stable/generated/torch.Tensor.zero_.html#torch.Tensor.zero_) is good for.\n",
    "\n",
    "---\n",
    "\n",
    "*In PyTorch, every method that **ends** with an **underscore (_)** makes changes **in-place**, meaning, they will **modify** the underlying variable.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.]], device='cuda:0'), tensor([0.], device='cuda:0'))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight.grad.zero_(), model.linear.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that seems a lot of work, don't worry, we'll soon automate this using an **optimizer**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yz6DZyBCs9sX"
   },
   "source": [
    "### Step 4: Update the Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UzaeN06rz5Ok"
   },
   "source": [
    "In the final step, we **use the gradients to update** the parameters. Since we are trying to **minimize** our **losses**, we **reverse the sign** of the gradient for the update.\n",
    "\n",
    "There is still another parameter to consider: the **learning rate**, denoted by the *Greek letter* **eta** (that looks like the letter **n**), which is the **multiplicative factor** that we need to apply to the gradient for the parameter update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmBroXjJ0L4I"
   },
   "source": [
    "**Parameters**:\n",
    "\n",
    "$$\n",
    "\\large b = b - \\eta \\frac{\\partial{\\text{MSE}}}{\\partial{b}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\large w = w - \\eta \\frac{\\partial{\\text{MSE}}}{\\partial{w}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LB6C0xscvB3o"
   },
   "source": [
    "### Optimizer:  learning the parameters step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cwemv-t3vIbJ"
   },
   "source": [
    "Updating parameters **manually** is probably fine for *two parameters*… but what if we had a **whole lot of them**?! We use one of PyTorch’s **optimizers**, like [SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) or [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam).\n",
    "\n",
    "---\n",
    "\n",
    "There are **many** optimizers, **SGD** is the most basic of them and **Adam** is one of the most popular. They achieve the same goal through, literally, **different paths**.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/AnalyticsVidhya_DataHour_PyTorch/main/images/sgd_adam_paths.png\" width=\"50%\"/>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "In the code below, we create a *Stochastic Gradient Descent* (SGD) optimizer to update our parameters **b** and **w**.\n",
    "\n",
    "---\n",
    "\n",
    "Don’t be fooled by the **optimizer’s** name: if we use **all training data** at once for the update the optimizer is performing a **batch** gradient descent, despite of its name.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tU6sjDLx0YUq"
   },
   "source": [
    "Let's start with a value of **0.1** (which is a relatively *high value*, as far as learning rates are concerned!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gOkDgwawBUR"
   },
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "lr = 1e-1\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.7645]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.8300], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lw8Sku2ev4ML"
   },
   "source": [
    "### step / zero_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimizer takes the **parameters** we want to update, the **learning rate** we want to use (and possibly many other hyper-parameters as well!) and **performs the updates** through its [**`step()`**](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9564]], device='cuda:0') tensor([-1.5652], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "yhat = model(x_train_batch)\n",
    "loss = loss_fn(yhat, y_train_batch)\n",
    "\n",
    "loss.backward()\n",
    "print(model.linear.weight.grad, model.linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9564]], device='cuda:0') tensor([-1.5652], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "optimizer.step()\n",
    "print(model.linear.weight.grad, model.linear.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3ljYgxLv52K"
   },
   "source": [
    "Besides, we also don’t need to zero the gradients one by one anymore. We just invoke the optimizer’s [**`zero_grad()`**](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad) method and that’s it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.]], device='cuda:0') tensor([0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "print(model.linear.weight.grad, model.linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.8602]], device='cuda:0', requires_grad=True) Parameter containing:\n",
      "tensor([0.9865], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.linear.weight, model.linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKF3qNFkIXIi"
   },
   "source": [
    "---\n",
    "\n",
    "<h2><b><i>\"Choose your learning rate wisely...\"</b></i></h2>\n",
    "\n",
    "<h3><i><b>The learning rate is the single most important hyper-parameter to tune when you are using Deep Learning models!</b></i></h3>\n",
    "\n",
    "What happens if I choose the learning rate **poorly**? Your model may **take too long to train** or **get stuck with a high loss** or, even worse, **diverge into an exploding loss**!\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/AnalyticsVidhya_DataHour_PyTorch/main/images/learning_rates.png\" width=\"50%\">\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BIUmWPfKtAm1"
   },
   "source": [
    "### Step 5: Rinse and Repeat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qvwtAKpd0frZ"
   },
   "source": [
    "Now we use the **updated parameters** to go back to **Step 1** and restart the process.\n",
    "\n",
    "Repeating this process over and over, for **many epochs**, is, in a nutshell, **training** a model.\n",
    "\n",
    "---\n",
    "\n",
    "An **epoch** is complete whenever **every point has been already used once for computing the loss**: \n",
    "- **batch** gradient descent: this is trivial, as it uses all points for computing the loss — **one epoch** is the same as **one update**\n",
    "- **stochastic** gradient descent: **one epoch** means **N updates**\n",
    "- **mini-batch** (of size n): **one epoch** has **N/n updates**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Let's put the previous pieces of code together and loop over many epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "# Defines number of epochs\n",
    "n_epochs = 200\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "n_features = 1\n",
    "n_outputs = 1\n",
    "model = nn.Sequential()\n",
    "model.add_module('linear', nn.Linear(n_features, n_outputs, bias=True))\n",
    "model.to(device)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_train_batch, y_train_batch in train_loader:\n",
    "        x_train_batch = x_train_batch.to(device)\n",
    "        y_train_batch = y_train_batch.to(device)\n",
    "        # Step 1\n",
    "        yhat = model(x_train_batch)\n",
    "\n",
    "        # Step 2\n",
    "        loss = loss_fn(yhat, y_train_batch)\n",
    "\n",
    "        # Step 3\n",
    "        loss.backward() \n",
    "\n",
    "        # Step 4\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BwwqUJWv5X0s",
    "outputId": "468da19d-a165-4d4e-806c-f033272bdab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[1.9695]], device='cuda:0')), ('linear.bias', tensor([1.0240], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RtH7USnV76Mz"
   },
   "source": [
    "Just keep in mind that, if you **don’t** use batch gradient descent (our example does), you’ll have to write an **inner loop** to perform the **four training steps** for either each **individual point** (**stochastic**) or **n points** (**mini-batch**). We’ll see a mini-batch example later down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ATtSr6pR8Oed"
   },
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ke4Bw5wI8Q5R"
   },
   "source": [
    "Just to make sure we haven’t done any mistakes in our code, we can use *Scikit-Learn’s Linear Regression* to fit the model and compare the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XUsUvW808QLv",
    "outputId": "27903aa6-3d39-41a1-f4fa-6558b30eb0f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02354075] [1.96896447]\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: do we get the same results as our gradient descent?\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They **match** — we have a *fully working implementation of linear regression* using PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cfde7qBr-dLd"
   },
   "source": [
    "Finally, we managed to successfully run our model and get the **resulting parameters**. Surely enough, they **match** the ones we got in our *Numpy*-only implementation.\n",
    "\n",
    "Let's take a look at the **loss** at the end of the training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qyrLZ7bm-fu3",
    "outputId": "6e292fe9-409b-4bad-e05e-7308b1385acb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shzUgjPH_hqq"
   },
   "source": [
    "What if we wanted to have it as a *Numpy* array? I guess we could just use **numpy()** again, right? (and **cpu()** as well, since our *loss* is in the `cuda` device..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "c-fSotmZ_s-9",
    "outputId": "1b52b633-68ab-4077-a4f5-53122695136e"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [96]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "loss.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Uwg76k-_6CH"
   },
   "source": [
    "What happened here? Unlike our *data tensors*, the **loss tensor** is actually computing gradients - and in order to use **numpy**, we need to [**detach()**](https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html#torch.Tensor.detach) that tensor from the computation graph first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qWBiGTaLAVfj",
    "outputId": "5372bb61-7f59-4f68-e4bd-f9fdeffd68ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.00488249, dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mVvbZu_6AZZU"
   },
   "source": [
    "This seems like **a lot of work**, there must be an easier way! And there is one indeed: we can use [**item()**](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html#torch.Tensor.item), for **tensors with a single element** or [**tolist()**](https://pytorch.org/docs/stable/generated/torch.Tensor.tolist.html#torch.Tensor.tolist) otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PCKOS0RlAx-U",
    "outputId": "fd2bee76-87d3-4b08-d575-d2cb009e4fe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004882486537098885 0.004882486537098885\n"
     ]
    }
   ],
   "source": [
    "print(loss.item(), loss.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7v_kLDpO_1kn"
   },
   "source": [
    "## Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6_ZPJMa_5DI"
   },
   "source": [
    "I believe this tutorial has **most of the necessary steps** one needs go to trough in order to **learn**, in a **structured** and **incremental** way, how to **train your first model using PyTorch**.\n",
    "\n",
    "Hopefully, after finishing working through all code in this post, you’ll be able to better appreciate and more easily work your way through PyTorch’s official [tutorials](https://pytorch.org/tutorials/).\n",
    "\n",
    "If you have any thoughts, comments or questions, please contact me on [LinkedIn](https://br.linkedin.com/in/dvgodoy) or [Twitter](https://twitter.com/dvgodoy).\n",
    "\n",
    "<h1><center>THANK YOU!</center></h1>\n",
    "\n",
    "<h2><center>\n",
    "<a href=\"https://dvgodoy.gumroad.com/l/pytorch/datahour\">Use the <strong>DATAHOUR</strong> coupon to<br>GET THE FIRST VOLUME<br>for\n",
    "    <strong>ONLY $1.95</strong>\n",
    "</a>\n",
    "</center></h2>\n",
    "\n",
    "<h2><center>\n",
    "    For more information on the <strong>paperback</strong> edition, please my book's official website:\n",
    "<br>\n",
    "<br>\n",
    "<a href=\"https://pytorchstepbystep.com\">\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/AnalyticsVidhya_DataHour_PyTorch/main/images/book_cover.png\" width=\"60%\">\n",
    "</p>\n",
    "<br>\n",
    "    https://pytorchstepbystep.com\n",
    "</a>\n",
    "</center></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Training Your First Model in PyTorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
